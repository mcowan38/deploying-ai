{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ef0627",
   "metadata": {},
   "source": [
    "# Embeddings via API\n",
    "\n",
    "In this notebook, we demonstrate how to obtain embeddings using OpenAI's API (offers an embedding service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d340a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets\n",
    "%load_ext dotenv \n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6890865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases to be embedded (from highly concentrated to less concentrated on the topic)\n",
    "# Want to obtain a representation that reflects the semantic similarity between these phrases; the similarity search should reflect meaning\n",
    "documents = [\n",
    "    \"The machine learning model predicts customer behavior based on historical data.\",\n",
    "    \"The machine learning model predicts user behavior using historical data.\",\n",
    "    \"A machine learning model predicts customer behavior from past data.\",\n",
    "    \"The predictive model uses historical customer data to forecast behavior.\",\n",
    "    \"Customer behavior is predicted by a data-driven machine learning system.\",\n",
    "    \"Historical data is analyzed to understand how customers behave.\",\n",
    "    \"A data science model analyzes past information to make predictions.\",\n",
    "    \"Business analysts study customer trends to support decision making.\",\n",
    "    \"Statistical techniques are used to interpret large datasets.\",\n",
    "    \"The weather forecast was inaccurate due to missing satellite data.\",\n",
    "    \"A novel explores human relationships in a small coastal town.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a07d1",
   "metadata": {},
   "source": [
    "OpenAI's text embeddings are available through the embeddings API. A key reference is the [Embeddings API documentation](https://platform.openai.com/docs/guides/embeddings).\n",
    "\n",
    "There are three models that we can choose from, depending on [the size of the hidden representation, latency, and cost](https://platform.openai.com/docs/guides/embeddings#embedding-models):\n",
    "\n",
    "# Depending on context, some models more appropriate\n",
    "+ `text-embedding-3-small` # original model; shorter context (e.g., emails)\n",
    "+ `text-embedding-3-large` # large documents\n",
    "+ `text-embedding-ada-002`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd77ae",
   "metadata": {},
   "source": [
    "A simple implementation would call the embeddings API for each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "\n",
    "# Function to call the API; getting rid of newlines/line breaks\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "\n",
    "embeddings = [get_embedding(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python list of embeddings; anything that goes into square parantheses is a list (will contain output of the get embedding function for each document in the document list)\n",
    "# If printed out you would get a list of vectors (each vector is a list of numbers representing the embedding for each document; contain floats that represent the position of the document in the embedding space)\n",
    "\n",
    "# Can check length of embeddings and length of documents to confirm they match (each document should have a corresponding embedding)\n",
    "len(embeddings), len(documents)\n",
    "\n",
    "# Once you have floats + arrays we can start thinking about similarity metrics\n",
    "import numpy as np\n",
    "\n",
    "embeddings_array = np.array(embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a6038",
   "metadata": {},
   "source": [
    "## A Note on Similarity\n",
    "\n",
    "One important characteristic of embeddings is that they can be used to measure the relatedness of text strings. To see this, we can plot a reduced forms of the embeddings using Principal Components Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2fa30",
   "metadata": {},
   "source": [
    "Similarity between two texts can be understood in two ways:\n",
    "\n",
    "+ Lexical similarity refers to similarity of the choice of words. For example, \"cats are fun\" and \"cats are furry\" are similar in that they have two words in common.\n",
    "+ Semantical similarity refers to similarity in the words meaning. For example, \"the bottle is empty\" and \"there is nothing in the bottle\" are similar in meaning, but the phrases do not have many words in common.\n",
    "\n",
    "Using count or tf-idf tokenization, we can calculate lexical similarity; using embeddings, we can compute (model-dependent) lexical similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775395af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reduce dimensionality of the embeddings to 2 dimensions for visualization purposes (via PCA)\n",
    "# Way of extracting linear correlation structure from a dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results; visualize the reduced embeddings in a 2D space; each point represents a document and the distance between points reflects their semantic similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"]).assign(label = documents)\n",
    "\n",
    "# Create the scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x', y='y', data=df, ax=ax)\n",
    "\n",
    "# Add labels\n",
    "texts = []\n",
    "for i, row in df.iterrows():\n",
    "    texts.append(ax.text(row['x'], row['y'], row['label'], fontsize=6))\n",
    "\n",
    "# Adjust text positions to avoid overlap\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black', lw=0.5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If we take embeddings, we can get a representation that will keep similar documents close together in the embedding space; we can use this for similarity search, clustering, or visualization; the PCA step is just for visualization purposes to reduce the high-dimensional embedding space to 2D while trying to preserve the structure of the data as much as possible.\n",
    "# Can use this mechanism for a search engine; if you have a query you can embed the query and then find the closest embeddings in your document collection to return relevant results; this is a common technique in information retrieval and natural language processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fd5e2",
   "metadata": {},
   "source": [
    "# Additional Note:\n",
    "- No guarantees on linear structure (standard is to use tSNE): https://www.jmlr.org/papers/v9/vandermaaten08a.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
